{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Очистка-текста\" data-toc-modified-id=\"Очистка-текста-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Очистка текста</a></span></li><li><span><a href=\"#Лемматизация-текста\" data-toc-modified-id=\"Лемматизация-текста-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Лемматизация текста</a></span></li><li><span><a href=\"#Разбивка-данных-на-выборки.\" data-toc-modified-id=\"Разбивка-данных-на-выборки.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Разбивка данных на выборки.</a></span></li><li><span><a href=\"#Векторизация-данных.\" data-toc-modified-id=\"Векторизация-данных.-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Векторизация данных.</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Словарь-параметров\" data-toc-modified-id=\"Словарь-параметров-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Словарь параметров</a></span></li></ul></li><li><span><a href=\"#Логистическая-регрессия.\" data-toc-modified-id=\"Логистическая-регрессия.-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Логистическая регрессия.</a></span></li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>LightGBM</a></span></li><li><span><a href=\"#Решающее-дерево.\" data-toc-modified-id=\"Решающее-дерево.-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Решающее дерево.</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from catboost import CatBoostClassifier \n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = 68756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nadya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nadya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Nadya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nadya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nadya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('toxic_comments.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропусков с данных нет, типы корректные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим копрус данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['text']#.values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Очистка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clear = [' '.join(re.sub(r\"[^a-zA-Z ]\", \" \", s.lower()).split()) for s in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизируем (длительность ~  минута)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(word):\n",
    "    dict = {\"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,        \n",
    "            \"R\": wordnet.ADV}\n",
    "\n",
    "    sign = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return dict.get(sign, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# pos-теги закомментировал, я состарюсь пока они посчитаются. \n",
    "# Если у вас есть на чем их посчитать, раскомментируйте эту строку пожалуйста, а вторую закомментируйте.\n",
    "\n",
    "#corpus_lemm = [' '.join([lemm.lemmatize(w, pos(w)) for w in s]) for s in [nltk.word_tokenize(st) for st in corpus_clear]]\n",
    "corpus_lemm = [' '.join([lemm.lemmatize(w) for w in s]) for s in [nltk.word_tokenize(st) for st in corpus_clear]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим обработанные данные в исходный датасет в столбец `text_lemm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_lemm'] = corpus_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      " 3   text_lemm   159292 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>congratulation from me a well use the tool wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry if the word nonsense wa offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1           1  D'aww! He matches this background colour I'm s...      0   \n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4           4  You, sir, are my hero. Any chance you remember...      0   \n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7           7  Your vandalism to the Matt Shirvington article...      0   \n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9           9  alignment on this subject and which are contra...      0   \n",
       "\n",
       "                                           text_lemm  \n",
       "0  explanation why the edits made under my userna...  \n",
       "1  d aww he match this background colour i m seem...  \n",
       "2  hey man i m really not trying to edit war it s...  \n",
       "3  more i can t make any real suggestion on impro...  \n",
       "4  you sir are my hero any chance you remember wh...  \n",
       "5  congratulation from me a well use the tool wel...  \n",
       "6       cocksucker before you piss around on my work  \n",
       "7  your vandalism to the matt shirvington article...  \n",
       "8  sorry if the word nonsense wa offensive to you...  \n",
       "9  alignment on this subject and which are contra...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбивка данных на выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['toxic']\n",
    "feature = df['text_lemm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = \\\n",
    "                train_test_split(feature, target, test_size=0.2, random_state=STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = \\\n",
    "                train_test_split(features_train, target_train, test_size=0.25, random_state=STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизируем данные с помощью TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "tf_idf = TfidfVectorizer(stop_words=list(stopwords), min_df=50) # min_df=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tf_idf_train = tf_idf.fit_transform(features_train)\n",
    "features_tf_idf_valid = tf_idf.transform(features_valid)\n",
    "features_tf_idf_test = tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**  \n",
    "Данные подготовлены. Очищены, лемматизированы, векторизированны. Можно обучать модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем обучать 3 модели:  \n",
    "- Логистическая регрессия.\n",
    "- Классификацию LightGBM.\n",
    "- Случайный лес."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Словарь параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "'grid_lr': {\n",
    "            'C': range(8, 14),\n",
    "            'solver' :  ['liblinear', 'newton-cg'],\n",
    "            'max_iter':[20, 500, 1000]\n",
    "           },\n",
    "'rand_lgbm': { # 0.7718260869565218\n",
    "            'min_child_samples': [20,25],\n",
    "            'num_leaves': [20,31],\n",
    "            'max_depth': range (12, 19, 2),  #16\n",
    "            'n_estimators': range(100, 2000, 450) #1020          \n",
    "             },\n",
    "'rand_dt': {\n",
    "            'min_samples_split': range (8,17),\n",
    "            'min_samples_leaf': range (8,17),\n",
    "            'max_depth': range (9,40,2),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(model, name):\n",
    "    print(f'Лучшая модель {name} - {model.best_estimator_}')\n",
    "    print(f'f1 лучшей модели {name} при кросс-валидации - {model.best_score_:.4f}')\n",
    "    print(f'f1 лучшей модели {name} на проверочной выборке - {f1_score(target_valid, model.best_estimator_.predict(features_tf_idf_valid)):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lg = LogisticRegression(n_jobs=-1, random_state=STATE).fit(features_tf_idf_train, target_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7258485639686684"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target_valid, model_lg.predict(features_tf_idf_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия с параметрами по умолчанию на показала f1 = 0.7258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем гиперпараметры ( ~1,5 минуты )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время подбора гиперпараметров GridSearchCV = 55с\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#print(model_lg)\n",
    "search = GridSearchCV(LogisticRegression(), param['grid_lr'], cv=5, scoring = 'f1', n_jobs=-1)\n",
    "search.fit(features_tf_idf_train, target_train)\n",
    "print(f\"Время подбора гиперпараметров GridSearchCV = {time.time() - start:.0f}с\")\n",
    "best_lg = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая модель линейной регрессии - LogisticRegression(C=9, max_iter=20, solver='liblinear')\n",
      "f1 лучшей модели линейной регрессии при кросс-валидации - 0.7621\n",
      "f1 лучшей модели линейной регрессии на проверочной выборке - 0.7528\n"
     ]
    }
   ],
   "source": [
    "print_result(search, 'линейной регрессии')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия с подобранными гиперпараметрами показала f1 = 0.7528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = LGBMClassifier(n_jobs=-1, random_state=STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_lgbm.fit(features_tf_idf_train, target_train)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7355236885097114"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target_valid, model_lgbm.predict(features_tf_idf_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM с параметрами по умолчанию показал f1 = 0.7355\n",
    "\n",
    "Подберем гиперпараметры ( ~ 20 минут )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lgbm_RS = RandomizedSearchCV(LGBMClassifier(n_jobs=-1), param['rand_lgbm'], cv=5, scoring = 'f1', \\\n",
    "                                                                                         random_state=STATE, n_jobs=-1)\n",
    "lgbm_RS.fit(features_tf_idf_train, target_train)\n",
    "print(f\"Время подбора гиперпараметров RandomizedSearchCV = {time.time() - start:.0f}с\")\n",
    "best_lgbm = lgbm_RS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(lgbm_RS, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM с подобранными гиперпараметрами показал f1 = 0.7516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решающее дерево."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решающее дерево (~ 2 минуты)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_dt = DecisionTreeClassifier(random_state=STATE).fit(features_tf_idf_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(target_valid, model_dt.predict(features_tf_idf_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Решающее дерево с параметрами по умолчанию на показало f1 = 0.6907"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем гиперпараметры ( ~ 1 минута )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время подбора гиперпараметров RandomizedSearchCV = 74с\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dt_RS = RandomizedSearchCV(DecisionTreeClassifier(), param['rand_dt'], cv=5, scoring = 'f1', \\\n",
    "                                                                                         random_state=STATE, n_jobs=-1)\n",
    "dt_RS.fit(features_tf_idf_train, target_train)\n",
    "print(f\"Время подбора гиперпараметров RandomizedSearchCV = {time.time() - start:.0f}с\")\n",
    "best_dt = dt_RS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(dt_RS, 'Решающее дерево')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решающее дерево с подобранными гиперпараметрами показало f1 = 0,6526"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**  \n",
    "Самый лучший результат на проверочной выборке показала модель Логистическая регрессия.  \n",
    "f1 модели = 0,7528  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим лучшую модель на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(target_test, best_lg.predict(features_tf_idf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На тестовой выборке f1 лучшей модели = 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим модель на адекватность константной моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_param = {\n",
    "                0:[\"constant\", \"  При нулевой константной модели f1 равно \"],\n",
    "                1:[\"constant\", \"При единичной константной модели f1 равно \"],\n",
    "                2:[\"uniform\", \"При случайной константной модели f1 равно \"]\n",
    "            }\n",
    "\n",
    "\n",
    "for i, s in dummy_param.items():\n",
    "    print(s[1], f1_score(target_test, DummyClassifier(strategy=s[0], constant=i).fit(features_tf_idf_train, target_train).predict(features_tf_idf_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три разных константных модели показали на тестовой выборке значение f1 значительно меньше демонстрируемого лучшей моделью.    \n",
    "Модель прошла проверку на адекватность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшая модель на тестовой выборке показала f1 = 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**  \n",
    "Во время выполнения проекта были выполнены следующие шаги:\n",
    "- получение данных\n",
    "- очистка текста\n",
    "- лемматизация текста\n",
    "- векторизация текста\n",
    "- обучение 3 моделей, и выбор лучшей по метрике f1\n",
    "- подбор гиперпараметров моделей с целью улучшить метрику\n",
    "- проверка лучшей модели на тестовой выборке\n",
    "- проверка лучшей модели на адекватность\n",
    "\n",
    "В итоге лучшая модель это Логистическая регрессия с подобранными гиперпараметрами.\n",
    "Лучшее значение метрики f1 = 0.76"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 50,
    "start_time": "2023-05-24T05:56:25.627Z"
   },
   {
    "duration": 2333,
    "start_time": "2023-05-24T05:59:56.866Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-24T05:59:59.202Z"
   },
   {
    "duration": 4030,
    "start_time": "2023-05-24T05:59:59.209Z"
   },
   {
    "duration": 250,
    "start_time": "2023-05-24T06:00:03.242Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.496Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.498Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.499Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.501Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.502Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.504Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.505Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.506Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.508Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.509Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.510Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.513Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.515Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.516Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.526Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.528Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.530Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.532Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.533Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.535Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.536Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.537Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.539Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.540Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.541Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.542Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.544Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.547Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.549Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.550Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.551Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-24T06:00:03.552Z"
   },
   {
    "duration": 48,
    "start_time": "2023-05-24T06:00:21.026Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.388px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
